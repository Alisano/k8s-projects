# Дано:
# - мультизональный кластер (три зоны), в котором пять нод
# - приложение требует около 5-10 секунд для инициализации
# - по результатам нагрузочного теста известно, что 4 пода справляются с пиковой нагрузкой
# - на первые запросы приложению требуется значительно больше ресурсов CPU, в дальнейшем потребление ровное в районе 0.1 CPU. По памяти всегда “ровно” в районе 128M memory
# - приложение имеет дневной цикл по нагрузке – ночью запросов на порядки меньше, пик – днём
# - хотим максимально отказоустойчивый deployment
# - хотим минимального потребления ресурсов от этого deployment’а

# Для удобства развернем окружение в отдельном неймспейсе
apiVersion: v1
kind: Namespace
metadata:
  name: test
---
# Как пример, развернем веб-сервер nginx
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: test
  labels:
    app: nginx
spec:
  replicas: 2 # Создадим 2 экземляра нашего пода для отказоустойчивости и минимального потребления ресурсов
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      affinity:
        podAntiAffinity: # Размещаем каждый под на отдельную зону и нод
          requiredDuringSchedulingIgnoredDuringExecution:
          # Ставим строгое требование по распределению подов для отказоустойчивости 
          # При максимальной нагрузке получаем 4 пода, размещенных на различных 4 нодах
          # В случае отказа работы какой-либо из нод, можно использовать условие preferredDuringScheduling чтобы разместить несколько под на одну ноду
          - labelSelector:
              matchLabels:
                app: nginx
            topologyKey: topology.kubernetes.io/hostname

          preferredDuringSchedulingIgnoredDuringExecution:
          # Ставим предпочтительное требование по размещению подов,
          # поскольку в пике нагрузки у нас 4 пода, а всего 3 зоны
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: nginx
              topologyKey: topology.kubernetes.io/zone
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
        resources:
          requests: # Задаем потребление ОЗУ и ЦПУ по условию задачи
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "1000m" # Учитываем случай всплесков CPU при кратковременной нагрузке
        # Проводим проверку работоспособности контейнера
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5 # Запускаем проверку работы контейнера по истичении 5 секунд после запуска контейнера
          periodSeconds: 5 # Проверяем работу контейнера через каждые 5 сек
          failureThreshold: 3 # Если проба падает 3 раза, то перезагружаем его
        # Проводим проверку что контейнер готов к принятию трафика
        readinessProbe:
          httpGet:
            path: /index.html
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
          failureThreshold: 3
---
# Задачу масштабированиям можно решать различными методами: 
# 1. оценивая метрики нагрузки ЦПУ, 2. числу запросов к приложению, 3. по расписанию
# Ниже будет рассмотрен первый метод
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-autoscaler
  namespace: test
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  minReplicas: 2 # Устанавливаем как раньше 2 пода для отказоустойчивости
  maxReplicas: 4 # И задаем 4 пода по условию задачи
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 10 # Средняя нарузка ЦПУ пода по условию задачи
  - type: Resource
    resource:
      name: memory
      target:
        type: AverageValue
        averageValue: 128Mi # Среднее потребление памяти пода по условию задачи
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60 # Ждем минуту перед масштабированием, чтобы учесть всплески нагрузки
---
# Создаем LoadBalancer, для распредления трафика между зонами
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  namespace: test
spec:
  type: LoadBalancer
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80 
      targetPort: 80
---
# Для подключения клиента к нашему nginx веб-сеерверу, воспользуемся ingress
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-ingress
  namespace: test
  labels:
    app: nginx
  annotations:
    nginx.ingress.kubernetes.io/app-root: /nginx
spec:
  ingressClassName: nginx
  rules:
    - host: localhost # Здесь можно писать любой другой домен
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: nginx-service
                port:
                  number: 80