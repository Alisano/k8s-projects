# Тестовое задание компании Mindbox на позицию SRE
В манифесте [nginx-manigest.yaml](https://github.com/Alisano/k8s-projects/blob/main/test_task/nginx-manifest.yaml) описаны процедуры разворачивания веб-сервера Nginx, с учетом следующих требований:
 - мультизональный кластер (три зоны), в котором пять нод
 - приложение требует около 5-10 секунд для инициализации
 - по результатам нагрузочного теста известно, что 4 пода справляются с пиковой нагрузкой
 - на первые запросы приложению требуется значительно больше ресурсов CPU, в дальнейшем потребление ровное в районе 0.1 CPU. По памяти всегда “ровно” в районе 128M memory
 - приложение имеет дневной цикл по нагрузке – ночью запросов на порядки меньше, пик – днём
 - хотим максимально отказоустойчивый deployment
 - хотим минимального потребления ресурсов от этого deployment’а
 
## Процесс поиска решений
В процессе написания манифеста, для проверки его работоспособности, был развернут на домашнем компьютере minikube кластер с 3 нодами (для меньшего потребления ресурсов). Основным источником информации была официальная документация kubernetes и minikube, а так же хабр-статьи. Для деплоя приложения был выбран веб-сервер nginx, широко представленный в различных примерах.

### Обеспечение отказоустройчивости deployment'a
Для обеспечения надежности deployment'а были использованы три идеи: 
- **каждый под** должен быть размещен на **отдельной ноде** и на **отдельной зоне**
- **с ростом нагрузки** на поды, необходимо **увеличивать число подов**
- при работе со **входящим трафиком перенаправлять** его на **менее нагруженную** зону

#### Задача распределения подов
Задачу распрделения подов по отдельным нодам и зонам можно решить двумя способами:
- Используя **affinity**-правила.
- Используя **topologySpreadConstraints**.
 
В представленном манифесте, из идей отказоустройчивости кластера, был реализован первый вариант решения посредством affinity-правил. Из которых было выбрано **AntiPodeAffinity** правило, обеспечивающее размещение подов на отдельные ноды и зоны. Дополнительно, также стоит сказать, что при описании affinity-правил, существует 2 способа их исполнения: 
1. **prefferedDuringSchedulingIgnoredDuringExecution** - желаемые требования, которые при невозможности выполнить affinity-правила, переводят поды в случайную ноду. Такой случай полезен если мы хотим разместить 4 пода на 3 зоны или когда одна из нод (зон) упали.
2. **requiredDuringSchedulingIgnoredDuringExecution** - строгие требования, которые переводят под в состояние **PENDING** в случае их не исполнения (например, не нашлось свободной ноды на зоне), что может отрицательно сказываться как на скорости работы веб-приложения, так и на его работоспособность.

Из представленных выше способов исполнения требований, **prefferedDuringSchedulingIgnoredDuringExecution** предпочтителен с точки зрения отказоустойчивости deployment'a.
В манифесте [nginx-manigest.yaml](https://github.com/Alisano/k8s-projects/blob/main/test_task/nginx-manifest.yaml) было решено использовать одновеременно оба типа требований: 
- **prefferedDuringSchedulingIgnoredDuringExecution** - для размешения подов на зоны. Так как число подов при максимальной нагрузке больше (4) чем число зон (3).
- **requiredDuringSchedulingIgnoredDuringExecution** - для размещения подов на ноды. Поскольку число под при максимальной нагрузке равно 4 (по условию задачи), а число worker-нод равно 4 (из учета, что 5 нода является master-нодой), то мы спокойно может разместить каждый под в отдельную ноду.
### Масштабирование deployment'a
Для масштабирования deployment'a был использован **HorizontalPodsAutoscaler** (далее **HPA**), для задания работы которого необходимо было выполнить ряд требований:
- Масштабирование должно происходить при изменении числа запросов к приложению (*например*, в ночное и дневное время суток).
- Необходимо учитывать кратковременные нагрузки на CPU, связанные с запросами пользователей приложению.

Отметим, что задача масштабирования deployment'а может решаться различными способами:
- Запуском **HPA** по расписанию (*например*, в дневное время запускаем 4 реплики подов, а в ночное 2).
- Измерением различных метрик:
    1. Отслеживанием числа запросов пользователей приложению.
    2. Потреблением ОЗУ и ЦПУ. 

Для более простой настройки **HPA** был выбран последний вариант, связанный с метриками ОЗУ и ЦПУ. Средние значения метрик были выбраны из условий тестового задания (0.1 CPU и 128 Mi ОЗУ). 

Дополнительно, для работы **HPA** в deployment были прописаны **limits** и **requests** ресурсы, из которых можно отдельно упомянуть **limits** по **cpu** установленным в порог 1000m cpu для учета нагрузки при первых запросах к приложению.

Чтобы решить проблему преждевременного масштабирования deployment'a при кратковременных нагрузках, было измененно поведение (**behavior**) **HPA** посредством параметра **stabilizationWindowSeconds**, задающего временной промежуток "задержки" масштабирования deployment'a для анализа метрик в более поздний промеждуток времени. 

### Перераспределение трафика трафика между зонами и нодами
Для решения задачи равномерного распределения трафика между нодами и зонами, предлагается использовать сервис типа **LoadBalancer**, а для связи трафика пользователя с данным сервисом - **ingress**. 

*Отмечу, что в тестовом кластере minikube, отдельно не настраивался ingress-контроллер, поскольку он подключается "из коробки" командами "minikube addons enable ingress".*
