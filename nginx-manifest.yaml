# Дано:
# - мультизональный кластер (три зоны), в котором пять нод
# - приложение требует около 5-10 секунд для инициализации
# - по результатам нагрузочного теста известно, что 4 пода справляются с пиковой нагрузкой
# - на первые запросы приложению требуется значительно больше ресурсов CPU, в дальнейшем потребление ровное в районе 0.1 CPU. По памяти всегда “ровно” в районе 128M memory
# - приложение имеет дневной цикл по нагрузке – ночью запросов на порядки меньше, пик – днём
# - хотим максимально отказоустойчивый deployment
# - хотим минимального потребления ресурсов от этого deployment’а

# Для удобства, вынесем наш проект в отдельный namespace с именем test
apiVersion: v1
kind: Namespace
metadata:
  name: test
---
# Создаем deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment # Для примера будем деплоить под с nginx контейнером, на его месте может быть любой другой
  namespace: test
  labels:
    app: nginx
spec:
  replicas: 2 # Создадим 2 экземляра нашего пода для отказоустойчивости и минимального потребления ресурсов
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      # Через affinity разместим поды отдельно друг от друга на различне ноды
      # Кроме этого, уставновим разрешение на размещение наших подов на указанные ноды
      affinity:
        podAntiAffinity: # Размещаем поды на разных нодах
          requiredDuringSchedulingIgnoredDuringExecution: # Делаем строгие требования по распределению подов на нодах
          # Такое требование обусловлено требованием отказоустойчивости
          # Кроме этого у нас при максимальной нагрузке 4 пода будут размещены на отдельных 4 нодах
            - labelSelector:
                matchLabels:
                  app: nginx
              topologyKey: topology.kubernetes.io/hostname 
        # Ниже представлен закоментированный отрывок манифеста, описывающий задание зон для размещения подов
        # Поскольку данный отрывок манефеста не тестировался, он был закоментирован
        # При необходимости, его можно раскоментировать, а значения зон задать в строчке values
        # nodeAffinity:
        #   requiredDuringSchedulingIgnoredDuringExecution:
        #     nodeSelectorTerms:
        #       - matchExpressions:
        #         - key: topology.kubernetes.io/zone
        #         operator: In
        #         values:
        #           - zone-1 # Тут (и в двух нижних строчках) меняем название зоны на используемую зону
        #           - zone-2
        #           - zone-3
      # Добавляем контейнеры
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80 # Чтобы не усложнять манифест (напирмер использованием ConfigMap), исплоьзуем порт по умолчанию
        # Задаем ограничения по ресурсам контейнера
        resources:
          requests: # Здесь описываем ресурсы используемые контейром большую часть времени
            memory: "128Mi" # Задаем 128 мб ОЗУ
            cpu: "100m" # Задаем потребление в 10 процентов CPU
          limits: # Здесь описываем ресурсы используемые контейром при максимальной нагрузке
            memory: "128Mi" # Можно дать больше памяти 
            cpu: "1000m" # Учитываем случай всплесков CPU
        # Проводим проверку работоспособности контейнера
        livenessProbe:
          httpGet:
            path: / # Стучимся на ngnix
            port: 80
          initialDelaySeconds: 5 # Запускаем проверку работы контейнера по истичении 5 секунд после запуска контейнера
          periodSeconds: 5 # Проверяем работу контейнера через каждые 5 сек
          failureThreshold: 3 # Если контейнер падает 3 раза, то перезагружаем его
        # Проводим проверку что контейнер готов к принятию трафика
        readinessProbe:
          httpGet:
            path: /index.html # Пробуем открыть страничку nginx'а
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
          failureThreshold: 3 # Если контейнер падает 3 раза, то отключаем его от сервиса
---
# Задаем горизонтальное масштабирование
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-autoscaler
  namespace: test
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  minReplicas: 2 # Устанавливаем как раньше 2 пода для отказоустойчивости
  maxReplicas: 4 # И задаем 4 пода по условию задачи
  # Задачу масшатбирования можно решать как минимум 4 методами. 
  # Первый и самый простой метод (представленный здесь) взять условие масштабирования 
  # по нагрузке ЦПУ в чуть-более 10 процентов (чтобы учесть всплески нагрузки при начале запросов приложению) 
  # если же нагрузка будет стабильно увеличиваться (например, днем), то система будет масшатбироваться до 4 подов
  # Второй подоход чуть более сложный, он основан на использоании механизма behavior. 
  # В нем мы можем задать временное окно (stabilizationWindowSeconds), чтобы учесть всплески нагрузки при начале запросов приложения
  # А также можем определить скорость масштабирования наших подов (в случаях scaleDown и scaleUp).
  # Третий метод заключается в испольлзовании метрики запросов (requests-per-second).
  # Четвертый подход - использовать CronJob и задавать ночью использовать меньше подов, а днем больше.
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 15 # Задаем среднее потребление пода чуть большее 10 процентов CPU, чтобы учесть случай всплесков CPU
  - type: Resource
    resource:
      name: memory
      target:
        type: AverageValue
        averageValue: 128Mi # Задаем среднее потребление памяти в 128 мб
---
# Создаем сервис - LoadBalancer, для распредления трафика между нодами (зонами)
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  namespace: test
spec:
  type: LoadBalancer
  selector:
    app: nginx
  ports:
    - protocol: TCP # Как пример, используем TCP протокол для подключению к nginx
      port: 80 
      targetPort: 80
---
# Для подключения клиента к нашему nginx веб-сеерверу, воспользуемся ingress
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-ingress
  namespace: test
  labels:
    app: nginx
  annotations:
    nginx.ingress.kubernetes.io/app-root: /nginx
spec:
  ingressClassName: nginx
  rules:
    - host: localhost # Здесь можно писать любой другой домен
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: nginx-service
                port:
                  number: 80